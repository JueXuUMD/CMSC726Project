{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T02:29:25.426583Z",
     "start_time": "2019-11-23T02:29:24.964519Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "from torchmps import MPS\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T02:30:48.036725Z",
     "start_time": "2019-11-23T02:30:48.031185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Miscellaneous initialization\n",
    "torch.manual_seed(0)\n",
    "start_time = time.time()\n",
    "\n",
    "# MPS parameters\n",
    "bond_dim      = 20\n",
    "adaptive_mode = False\n",
    "periodic_bc   = False\n",
    "\n",
    "# Training parameters\n",
    "num_train  = 2000\n",
    "num_test   = 1000\n",
    "batch_size = 100\n",
    "num_epochs = 20\n",
    "learn_rate = 1e-4\n",
    "l2_reg     = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T02:30:50.312822Z",
     "start_time": "2019-11-23T02:30:48.750558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Training on 2000 MNIST images \n",
      "(testing on 1000) for 20 epochs\n",
      "Maximum MPS bond dimension = 20\n",
      " * Fixed bond dimensions\n",
      " * Open boundary conditions\n",
      "Using Adam w/ learning rate = 1.0e-04\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the MPS module\n",
    "mps = MPS(input_dim=28**2, output_dim=10, bond_dim=bond_dim, \n",
    "          adaptive_mode=adaptive_mode, periodic_bc=periodic_bc)\n",
    "\n",
    "# Set our loss function and optimizer\n",
    "loss_fun = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mps.parameters(), lr=learn_rate, \n",
    "                             weight_decay=l2_reg)\n",
    "\n",
    "# Get the training and test sets\n",
    "## FASHIONMNIST\n",
    "# transform = transforms.ToTensor()\n",
    "# train_set = datasets.FashionMNIST('./fashionmnist', download=True, transform=transform)\n",
    "# test_set = datasets.FashionMNIST('./fashionmnist', download=True, transform=transform, \n",
    "#                           train=False)\n",
    "\n",
    "## MNIST\n",
    "# train_set = datasets.MNIST('./mnist', download=True, transform=transform)\n",
    "# test_set = datasets.MNIST('./mnist', download=True, transform=transform, \n",
    "#                           train=False)\n",
    "\n",
    "## Cifar\n",
    "transform = transforms.ToTensor()\n",
    "train_set = datasets.CIFAR10('./cifar', download=True, transform=transform)\n",
    "test_set = datasets.CIFAR10('./cifar', download=True, transform=transform, \n",
    "                          train=False)\n",
    "\n",
    "# Put MNIST data into dataloaders\n",
    "samplers = {'train': torch.utils.data.SubsetRandomSampler(range(num_train)),\n",
    "            'test': torch.utils.data.SubsetRandomSampler(range(num_test))}\n",
    "loaders = {name: torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "           sampler=samplers[name], drop_last=True) for (name, dataset) in \n",
    "           [('train', train_set), ('test', test_set)]}\n",
    "num_batches = {name: total_num // batch_size for (name, total_num) in\n",
    "               [('train', num_train), ('test', num_test)]}\n",
    "\n",
    "print(f\"Training on {num_train} MNIST images \\n\"\n",
    "      f\"(testing on {num_test}) for {num_epochs} epochs\")\n",
    "print(f\"Maximum MPS bond dimension = {bond_dim}\")\n",
    "print(f\" * {'Adaptive' if adaptive_mode else 'Fixed'} bond dimensions\")\n",
    "print(f\" * {'Periodic' if periodic_bc else 'Open'} boundary conditions\")\n",
    "print(f\"Using Adam w/ learning rate = {learn_rate:.1e}\")\n",
    "if l2_reg > 0:\n",
    "    print(f\" * L2 regularization = {l2_reg:.2e}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-23T02:30:50.412235Z",
     "start_time": "2019-11-23T02:30:50.362920Z"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[100, 784]' is invalid for input of size 307200",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-c539562006a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# Call our MPS to get logit scores and predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[100, 784]' is invalid for input of size 307200"
     ]
    }
   ],
   "source": [
    "# Let's start training!\n",
    "for epoch_num in range(1, num_epochs+1):\n",
    "    running_loss = 0.\n",
    "    running_acc = 0.\n",
    "\n",
    "    for inputs, labels in loaders['train']:\n",
    "        inputs, labels = inputs.view([batch_size, 28**2]), labels.data\n",
    "\n",
    "        # Call our MPS to get logit scores and predictions\n",
    "        scores = mps(inputs)\n",
    "        _, preds = torch.max(scores, 1)\n",
    "\n",
    "        # Compute the loss and accuracy, add them to the running totals\n",
    "        loss = loss_fun(scores, labels)\n",
    "        with torch.no_grad():\n",
    "            accuracy = torch.sum(preds == labels).item() / batch_size\n",
    "            running_loss += loss\n",
    "            running_acc += accuracy\n",
    "\n",
    "        # Backpropagate and update parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"### Epoch {epoch_num} ###\")\n",
    "    print(f\"Average loss:           {running_loss / num_batches['train']:.4f}\")\n",
    "    print(f\"Average train accuracy: {running_acc / num_batches['train']:.4f}\")\n",
    "    \n",
    "    # Evaluate accuracy of MPS classifier on the test set\n",
    "    with torch.no_grad():\n",
    "        running_acc = 0.\n",
    "\n",
    "        for inputs, labels in loaders['test']:\n",
    "            inputs, labels = inputs.view([batch_size, 28**2]), labels.data\n",
    "\n",
    "            # Call our MPS to get logit scores and predictions\n",
    "            scores = mps(inputs)\n",
    "            _, preds = torch.max(scores, 1)\n",
    "            running_acc += torch.sum(preds == labels).item() / batch_size\n",
    "\n",
    "    print(f\"Test accuracy:          {running_acc / num_batches['test']:.4f}\")\n",
    "    print(f\"Runtime so far:         {int(time.time()-start_time)} sec\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": true,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
